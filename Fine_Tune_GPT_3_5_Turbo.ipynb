{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1AvbBa4eBhSFZbGT2ZO699R5kwH9NvD3P",
      "authorship_tag": "ABX9TyOad0RcaZiIXrrbUKn5X9CS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khaledsoudy-1/gpt-3-5-turbo-finetuning-tutorial/blob/main/Fine_Tune_GPT_3_5_Turbo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ¤– **Introduction to Fine-Tuning GPT-3.5 Turbo**\n",
        "\n",
        "Fine-tuning is like giving a pre-trained model, such as **GPT-3.5** Turbo, a specialized \"training session\" to perform better on your specific tasks. Instead of starting from zero, you're building on a powerful model that already understands a lot about language, and guiding it to excel in your area of interest. ğŸ¯\n",
        "\n",
        "### **Why Fine-Tune GPT-3.5 Turbo?**\n",
        "\n",
        "- **ğŸ”¥ Boost Performance:** Fine-tuning can help the model perform much better on tasks that matter to youâ€”whether it's answering questions, generating creative content, or even mastering a specific language (like Arabic!).\n",
        "- **ğŸ¨ Task Specialization:** Want GPT-3.5 Turbo to get even better at something specific, like technical writing, summarizing articles, or even customer service? Fine-tuning is the key to unlocking that potential.\n",
        "- **ğŸ’¡ Personalization:** Customize GPT-3.5 Turbo to match your tone, style, and preferences, making its responses feel more personal and on-point for your needs.\n",
        "\n",
        "### **How Does Fine-Tuning Work?**\n",
        "\n",
        "At a high level, fine-tuning is like showing GPT-3.5 Turbo a set of examples that guide it to respond better to your particular type of task. Think of it as teaching a model to become more \"in tune\" with your unique requirements. ğŸ¤–âœ¨\n",
        "\n",
        "Hereâ€™s a quick rundown of how the magic happens:\n",
        "\n",
        "1. **Prepare Your Dataset ğŸ“‚:** The first step is gathering a set of input-output examples that are specific to what you want the model to learn. These examples could be prompts (questions or tasks) and the desired responses (answers or completions).\n",
        "   \n",
        "2. **Upload Your Dataset ğŸ“¤:** Once you've got your dataset, youâ€™ll upload it to OpenAIâ€™s API. The data should be in JSON format, with each entry containing a `prompt` and a `completion`â€”essentially, the question and the answer.\n",
        "\n",
        "3. **Fine-Tune the Model ğŸ’ª:** After uploading your dataset, you can kick off the fine-tuning process! Youâ€™ll specify the model (like `gpt-3.5-turbo`), point to your dataset, and configure the fine-tuning job.\n",
        "\n",
        "4. **Evaluate the Model ğŸ§:** Once the fine-tuning is done, itâ€™s time to see how the model performs! Test it on some new examples to make sure itâ€™s giving you the responses you need.\n",
        "\n",
        "5. **Refine & Repeat ğŸ”„:** Donâ€™t worry if the first result isnâ€™t perfectâ€”fine-tuning is an iterative process. Refine your dataset, tweak the parameters, and continue improving the model!\n",
        "\n",
        "In the next sections, we'll dive deeper into each step, showing you exactly **how to fine-tune GPT-3.5-Turbo**, step by step. Ready to get started? Letâ€™s go! ğŸš€\n"
      ],
      "metadata": {
        "id": "Po0x3ZWx8aIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸš€ Step 1: Install the Required Library\n",
        "\n",
        "First, we need to install the `openai` library to access OpenAIâ€™s API."
      ],
      "metadata": {
        "id": "qzNchn8O9MlQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DvZCOZvyrw-b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a13fc4ac-aa74-4ba1-f0ca-47363b64b4d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n"
          ]
        }
      ],
      "source": [
        "! pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“² Step 2: Connect to OpenAIâ€™s API\n",
        "\n",
        "After installing, import the `OpenAI` class and set up a client to interact with the API. Youâ€™ll need an API key from OpenAI."
      ],
      "metadata": {
        "id": "3t1JIB1k9VHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('OpenAI_API')\n",
        "\n",
        "if api_key:\n",
        "  print(f\"OpenAI API Key: {api_key[:5]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVmcSG6qF7gJ",
        "outputId": "3759dfd3-5bd4-42dc-92b4-02ddd4a04554"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API Key: sk-pr...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=api_key\n",
        ")"
      ],
      "metadata": {
        "id": "uwMcVe1uH_Hw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **`userdata`:** This module from Google Colab allows us to securely access secrets stored within Colab's environment.\n",
        "- **`api_key`**: Your private key to access the OpenAI API.\n",
        "- **`OpenAI`**: The main class to interact with OpenAIâ€™s services.\n",
        "- TheÂ **`api_key`**Â is passed toÂ **`OpenAI`**Â so it can authenticate with the **OpenAI API** for making requests, such as starting a fine-tuning job or generating text.\n",
        "\n",
        "> âš ï¸ **Tip**: Never share your API key publicly to keep your account secure!\n",
        ">"
      ],
      "metadata": {
        "id": "9ihzZrdm9hrJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“‚ Step 3: Load Data from Google Drive\n",
        "\n",
        "Since Colab sessions reset, storing data on Google Drive makes it easily accessible. Mount Google Drive so Colab can interact with it."
      ],
      "metadata": {
        "id": "jF8ykujq9o4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqQbqCdkJ7KK",
        "outputId": "6016aa6d-7e95-4d6f-9e5a-e5b794525fda"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ”— Step 4: Set File Paths\n",
        "\n",
        "Specify paths to your training and validation files in JSONL format. These files contain examples to help the model learn during fine-tuning."
      ],
      "metadata": {
        "id": "tmPWom_E9vP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_file = \"/content/drive/MyDrive/Colab Notebooks/04- Fine-Tune GPT 3.5 Turbo /training_data.jsonl\"\n",
        "\n",
        "validation_file = \"/content/drive/MyDrive/Colab Notebooks/04- Fine-Tune GPT 3.5 Turbo /validation_data.jsonl\""
      ],
      "metadata": {
        "id": "WL_mry_FLGSZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“„ Step 5: Upload Files to OpenAI\n",
        "\n",
        "Upload the files to OpenAIâ€™s servers, setting the purpose to `\"fine-tune\"` so they know itâ€™s for training."
      ],
      "metadata": {
        "id": "cftjMBpK9y9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_file_obj = client.files.create(\n",
        "  file=open(training_file, \"rb\"),\n",
        "  purpose=\"fine-tune\"\n",
        ")\n",
        "\n",
        "\n",
        "validation_file_obj = client.files.create(\n",
        "  file=open(validation_file, \"rb\"),\n",
        "  purpose=\"fine-tune\"\n",
        ")\n",
        "\n",
        "print(f\"Training File Object: {training_file_obj}\\n\")\n",
        "print(f\"validation File Object: {validation_file_obj}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txYE7YfnLWQt",
        "outputId": "d2e92351-700b-45c1-aaab-13fdc061629c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training File Object: FileObject(id='file-tkKgy3dx3YbXvm16SUEGRYgT', bytes=5181, created_at=1731503978, filename='training_data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)\n",
            "\n",
            "validation File Object: FileObject(id='file-g7CWUEgXa7PDgBvxZPUbZxc1', bytes=2685, created_at=1731503978, filename='validation_data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **`client.files.create()`**: This method **uploads a file** to OpenAIâ€™s server.\n",
        "- **`open(file_name, \"rb\")`**Â opens the data file (stored in the variableÂ **training_file** or **validation_file**) in **binary read mode**.\n",
        "- **`file=`**Â parameter is where you specify the file to upload.\n",
        "- **`purpose=\"fine-tune\"`**: This argument tells OpenAI's API that this file is meant specifically for fine-tuning, so the API processes it accordingly."
      ],
      "metadata": {
        "id": "AMiBeMsm-ENC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_file_id = training_file_obj.id\n",
        "\n",
        "validation_file_id = validation_file_obj.id\n",
        "\n",
        "print(f\"Training File ID: {training_file_id}\\n\")\n",
        "print(f\"validation File ID: {validation_file_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKb1htqVMFTS",
        "outputId": "bc8a9e23-9dce-444c-ca66-03dbe3a29b0c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training File ID: file-tkKgy3dx3YbXvm16SUEGRYgT\n",
            "\n",
            "validation File ID: file-g7CWUEgXa7PDgBvxZPUbZxc1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ”§ Step 6: Start the Fine-Tuning Job\n",
        "\n",
        "Now that weâ€™ve uploaded the files, we can start the fine-tuning job by providing the `training_file_id`, `validation_file_id`, and model name (`\"gpt-3.5-turbo\"`)."
      ],
      "metadata": {
        "id": "HWX-qhM2-GpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuning_job_response = client.fine_tuning.jobs.create(\n",
        "  training_file=training_file_id,\n",
        "  validation_file=validation_file_id,\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  hyperparameters={\n",
        "      \"n_epochs\": 15,\n",
        "      \"batch_size\": 3,\n",
        "      \"learning_rate_multiplier\": 0.3,\n",
        "  },\n",
        ")\n",
        "\n",
        "fine_tuning_job_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KezrVth8P6ur",
        "outputId": "cade35c8-aa67-46e0-f7a4-6d0820d3897a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FineTuningJob(id='ftjob-XdWROGbBqI1j9vRFjDdX3jst', created_at=1731503980, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=15, batch_size=3, learning_rate_multiplier=0.3), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-mgfjI7HyLPZIak99Sy4VvIW5', result_files=[], seed=2061283848, status='validating_files', trained_tokens=None, training_file='file-tkKgy3dx3YbXvm16SUEGRYgT', validation_file='file-g7CWUEgXa7PDgBvxZPUbZxc1', estimated_finish=None, integrations=[], user_provided_suffix=None)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **`client.fine_tuning.jobs.create(...)`**: This initiates a fine-tuning job using OpenAIâ€™s API.\n",
        "- **`hyperparameters={...}`**: A dictionary specifying training settings for fine-tuning:\n",
        "- **`n_epochs`**: Sets the number of times the model will iterate over the training data.\n",
        "- **`batch_size`**: Number of examples processed at once (higher numbers can be faster but require more memory).\n",
        "- **`learning_rate_multiplier`**: Adjusts the learning rate to control how quickly the model learns."
      ],
      "metadata": {
        "id": "G_15ISFv-SR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the Job ID\n",
        "job_id = fine_tuning_job_response.id\n",
        "\n",
        "# Get the Job Status\n",
        "job_status = fine_tuning_job_response.status\n",
        "\n",
        "print(f\"Fine Tuning Job ID: {job_id}\\n\")\n",
        "print(f\"Fine Tuning Job Status: {job_status}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpnYaunDRsVs",
        "outputId": "38759c63-3263-4a97-8f22-eaafc7760c23"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine Tuning Job ID: ftjob-XdWROGbBqI1j9vRFjDdX3jst\n",
            "\n",
            "Fine Tuning Job Status: validating_files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### â—â— This is step is **crusial** If Google Colab's Run-Time shuts off."
      ],
      "metadata": {
        "id": "wMlLHHaqolqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jobs = client.fine_tuning.jobs.list()  # List all fine-tuning jobs\n",
        "\n",
        "for job in jobs.data:\n",
        "    print(f\"Job ID: {job.id} - Status: {job.status}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKYJhdI3opq6",
        "outputId": "cb7aa421-f102-4eda-f67b-9cbc9f1efdb5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job ID: ftjob-XdWROGbBqI1j9vRFjDdX3jst - Status: succeeded\n",
            "Job ID: ftjob-HOzhngnH6aESL3yzmLq6vD98 - Status: succeeded\n",
            "Job ID: ftjob-EEzs9MqqAy53nnpQ5eJt6Hdi - Status: succeeded\n",
            "Job ID: ftjob-tYQ8qGwhoAvSN4mfK8mCDehn - Status: succeeded\n",
            "Job ID: ftjob-FnSdcTx98w5ypCKS1EsiG1IT - Status: succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# â³ Step 7: Monitor Training Progress\n",
        "\n",
        "Using Pythonâ€™s `signal` module, set up a handler to monitor the events of a fine-tuning job and gracefully handle interruptions, such as the user stopping the script (by pressing **Ctrl+C**).."
      ],
      "metadata": {
        "id": "e5ehyWMJ-Xw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import signal\n",
        "import datetime\n",
        "\n",
        "job_id = \"ftjob-XdWROGbBqI1j9vRFjDdX3jst\"       # In Case Google Colab run-time goes off\n",
        "\n",
        "# Create a function for Handling interruptions (e.g., Ctrl+C)\n",
        "def signal_handler(sig, frame):\n",
        "  job_status = client.fine_tuning.jobs.retrieve(fine_tuning_job_id=job_id).status          # Status of the job ('succeeded' or 'failed')\n",
        "  print(f\"Stream interrupted. The fine-tuning job status is {job_status}\")\n",
        "  return\n",
        "\n",
        "# Set up signal handler for interruption\n",
        "signal.signal(signal.SIGINT, signal_handler)\n",
        "\n",
        "print(f\"Streaming events for the fine-tuning job: {job_id}\\n\")\n",
        "\n",
        "# Fetch and display events for the fine-tuning job\n",
        "try:\n",
        "  events = client.fine_tuning.jobs.list_events(fine_tuning_job_id=job_id)\n",
        "\n",
        "  for event in events:\n",
        "    event_time = datetime.datetime.fromtimestamp(event.created_at)\n",
        "    event_message = event.message\n",
        "\n",
        "    print(f\"{event_time} - {event_message}\")       # Print time and message for each event\n",
        "\n",
        "except Exception as e:\n",
        "  print(f\"Stream Interrupted (Client Disconnected): {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMVJ7657TAEJ",
        "outputId": "6db41baf-7ef8-46a1-e8aa-c7953afbcff5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streaming events for the fine-tuning job: ftjob-XdWROGbBqI1j9vRFjDdX3jst\n",
            "\n",
            "2024-11-13 13:24:38 - The job has successfully completed\n",
            "2024-11-13 13:24:35 - New fine-tuned model created\n",
            "2024-11-13 13:24:35 - Checkpoint created at step 54\n",
            "2024-11-13 13:24:35 - Checkpoint created at step 51\n",
            "2024-11-13 13:24:21 - Step 55/55: training loss=0.63, validation loss=0.59\n",
            "2024-11-13 13:24:21 - Step 54/55: training loss=0.67, validation loss=0.58, full validation loss=0.60\n",
            "2024-11-13 13:24:16 - Step 53/55: training loss=0.90, validation loss=0.68\n",
            "2024-11-13 13:24:14 - Step 52/55: training loss=0.60, validation loss=0.56\n",
            "2024-11-13 13:24:14 - Step 51/55: training loss=0.73, validation loss=0.70, full validation loss=0.60\n",
            "2024-11-13 13:24:09 - Step 50/55: training loss=0.80, validation loss=0.56\n",
            "2024-11-13 13:24:07 - Step 49/55: training loss=0.61, validation loss=0.74\n",
            "2024-11-13 13:24:07 - Step 48/55: training loss=0.63, validation loss=0.59, full validation loss=0.60\n",
            "2024-11-13 13:24:02 - Step 47/55: training loss=0.69, validation loss=0.58\n",
            "2024-11-13 13:24:00 - Step 46/55: training loss=0.88, validation loss=0.68\n",
            "2024-11-13 13:24:00 - Step 45/55: training loss=0.54, validation loss=0.56, full validation loss=0.60\n",
            "2024-11-13 13:23:55 - Step 44/55: training loss=0.79, validation loss=0.70\n",
            "2024-11-13 13:23:53 - Step 43/55: training loss=0.78, validation loss=0.56\n",
            "2024-11-13 13:23:53 - Step 42/55: training loss=0.60, validation loss=0.74, full validation loss=0.60\n",
            "2024-11-13 13:23:48 - Step 41/55: training loss=0.60, validation loss=0.60\n",
            "2024-11-13 13:23:46 - Step 40/55: training loss=0.85, validation loss=0.58\n",
            "2024-11-13 13:23:46 - Step 39/55: training loss=0.67, validation loss=0.68, full validation loss=0.61\n",
            "2024-11-13 13:23:40 - Step 38/55: training loss=0.54, validation loss=0.57\n",
            "2024-11-13 13:23:38 - Step 37/55: training loss=0.81, validation loss=0.71\n",
            "2024-11-13 13:23:38 - Step 36/55: training loss=0.78, validation loss=0.57, full validation loss=0.61\n",
            "2024-11-13 13:23:33 - Step 35/55: training loss=0.83, validation loss=0.75\n",
            "2024-11-13 13:23:31 - Step 34/55: training loss=0.55, validation loss=0.60\n",
            "2024-11-13 13:23:31 - Step 33/55: training loss=0.64, validation loss=0.59, full validation loss=0.62\n",
            "2024-11-13 13:23:26 - Step 32/55: training loss=0.72, validation loss=0.70\n",
            "2024-11-13 13:23:24 - Step 31/55: training loss=0.85, validation loss=0.58\n",
            "2024-11-13 13:23:24 - Step 30/55: training loss=0.70, validation loss=0.72, full validation loss=0.63\n",
            "2024-11-13 13:23:19 - Step 29/55: training loss=0.93, validation loss=0.60\n",
            "2024-11-13 13:23:19 - Step 28/55: training loss=0.67, validation loss=0.78\n",
            "2024-11-13 13:23:17 - Step 27/55: training loss=0.74, validation loss=0.63, full validation loss=0.64\n",
            "2024-11-13 13:23:15 - Step 26/55: training loss=0.75, validation loss=0.62\n",
            "2024-11-13 13:23:12 - Step 25/55: training loss=0.81, validation loss=0.73\n",
            "2024-11-13 13:23:10 - Step 24/55: training loss=0.70, validation loss=0.62, full validation loss=0.66\n",
            "2024-11-13 13:23:07 - Step 23/55: training loss=0.76, validation loss=0.76\n",
            "2024-11-13 13:23:05 - Step 22/55: training loss=0.64, validation loss=0.65\n",
            "2024-11-13 13:23:03 - Step 21/55: training loss=0.81, validation loss=0.83, full validation loss=0.68\n",
            "2024-11-13 13:22:58 - Step 20/55: training loss=0.95, validation loss=0.68\n",
            "2024-11-13 13:22:58 - Step 19/55: training loss=0.86, validation loss=0.67\n",
            "2024-11-13 13:22:55 - Step 18/55: training loss=0.82, validation loss=0.77, full validation loss=0.71\n",
            "2024-11-13 13:22:53 - Step 17/55: training loss=0.92, validation loss=0.69\n",
            "2024-11-13 13:22:51 - Step 16/55: training loss=0.95, validation loss=0.82\n",
            "2024-11-13 13:22:48 - Step 15/55: training loss=1.03, validation loss=0.73, full validation loss=0.75\n",
            "2024-11-13 13:22:46 - Step 14/55: training loss=0.71, validation loss=0.90\n",
            "2024-11-13 13:22:44 - Step 13/55: training loss=0.94, validation loss=0.78\n",
            "2024-11-13 13:22:41 - Step 12/55: training loss=0.94, validation loss=0.81, full validation loss=0.82\n",
            "2024-11-13 13:22:39 - Step 11/55: training loss=1.21, validation loss=0.88\n",
            "2024-11-13 13:22:36 - Step 10/55: training loss=0.89, validation loss=0.87\n",
            "2024-11-13 13:22:34 - Step 9/55: training loss=0.92, validation loss=0.95, full validation loss=0.91\n",
            "2024-11-13 13:22:31 - Step 8/55: training loss=1.10, validation loss=0.96\n",
            "2024-11-13 13:22:29 - Step 7/55: training loss=1.20, validation loss=1.11\n",
            "2024-11-13 13:22:27 - Step 6/55: training loss=1.17, validation loss=1.02, full validation loss=1.04\n",
            "2024-11-13 13:22:24 - Step 5/55: training loss=1.31, validation loss=1.10\n",
            "2024-11-13 13:22:22 - Step 4/55: training loss=1.07, validation loss=1.08\n",
            "2024-11-13 13:22:20 - Step 3/55: training loss=1.04, validation loss=1.15, full validation loss=1.15\n",
            "2024-11-13 13:22:17 - Step 2/55: training loss=1.35, validation loss=1.21\n",
            "2024-11-13 13:22:15 - Step 1/55: training loss=1.49, validation loss=1.20\n",
            "2024-11-13 13:20:06 - Fine-tuning job started\n",
            "2024-11-13 13:20:04 - Files validated, moving job to queued state\n",
            "2024-11-13 13:19:40 - Validating training file: file-tkKgy3dx3YbXvm16SUEGRYgT and validation file: file-g7CWUEgXa7PDgBvxZPUbZxc1\n",
            "2024-11-13 13:19:40 - Created fine-tuning job: ftjob-XdWROGbBqI1j9vRFjDdX3jst\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **`signal`:** for handling interrupts (like Ctrl+C).\n",
        "- `datetime`: for converting timestamps to readable dates\n",
        "- **`signal_handler(sig, frame)`**: Defines a function to handle signals likeÂ **`SIGINT`**Â (**Ctrl+C**). If the script is interrupted, it retrieves the current job status and prints it, letting the user know the jobâ€™s state.\n",
        "- **`sig`**Â andÂ **`frame`**Â areÂ **parameters of the signal handler function**, and theÂ **`signal`**Â module passes them automatically when a signal (likeÂ **`SIGINT`**) is received.\n",
        "- **`signal.signal(signal.SIGINT, signal_handler)`**: Tells Python to callÂ **`signal_handler`**Â when aÂ **`SIGINT`**Â (interrupt) signal is received, helping manage unexpected terminations gracefully.\n",
        "- **`client.fine_tuning.jobs.list_events(job_id)`**: Retrieves a list of events related to the fine-tuning job identified byÂ **`job_id`**."
      ],
      "metadata": {
        "id": "7Gev_dEe-5J-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# â± Step 8: Wait for Completion\n",
        "\n",
        "Now, check if the job has finished by periodically retrieving its **status**."
      ],
      "metadata": {
        "id": "-DAP9xqL-8HI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "job_id = \"ftjob-XdWROGbBqI1j9vRFjDdX3jst\"                 # In Case Google Colab run-time goes off\n",
        "\n",
        "# Retrieve the status of the fine-tuning job\n",
        "job_status = client.fine_tuning.jobs.retrieve(fine_tuning_job_id=job_id).status\n",
        "\n",
        "if job_status not in ['succeeded', 'failed']:\n",
        "  print(f\"Job is still running. Current status: {job_status}. Waiting...\")\n",
        "\n",
        "  while job_status not in ['succeeded', 'failed']:\n",
        "    time.sleep(2)\n",
        "    job_status = client.fine_tuning.jobs.retrieve(fine_tuning_job_id=job_id).status          # Check status again\n",
        "    print(f\"Job Status: {job_status}\")\n",
        "\n",
        "else:\n",
        "  print(f\"Fine-tune Job {job_id} finished with status {job_status}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2lduAqw29Nu",
        "outputId": "457ffda5-8b07-4149-d189-3e37258c1fae"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tune Job ftjob-XdWROGbBqI1j9vRFjDdX3jst finished with status succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **`client.fine_tuning.jobs.retrieve(job_id).status`**: This retrieves the status of the fine-tuning job using theÂ **`job_id`**. It makes an API call to get information about the fine-tuning job, and theÂ **`.status`**Â accesses the status of the job (e.g., â€**pending**â€, â€œ**in_progress**â€, â€œ**succeeded**â€, â€œ**failed**â€).\n",
        "- **`if status not in [\"succeeded\", \"failed\"]:`**: This checks if the current status of the job is not one of the terminal statuses (**Succeeded** or **Failed**). Terminal statuses mean that the job has completed (either successfully or with failure).\n",
        "- **`while status not in [\"succeeded\", \"failed\"]:`**: This loop will keep running as long as the job status is neitherÂ **Succeeded** nor **Failed**. It continuously checks the job status.\n",
        "- **`time.sleep(2)`**: This pauses the execution of the code for 2 seconds before checking the status again. This helps avoid overwhelming the API with continuous requests.\n",
        "- **`status = client.fine_tuning.jobs.retrieve(job_id).status`**: After the 2-second pause, the status of the job is checked again.\n",
        "- **`else:`**: If the job has finished (i.e., itâ€™s eitherÂ Â **Succeeded** or **Failed**), the code will break out of theÂ **`while`**Â loop and print the final status of the fine-tuning job."
      ],
      "metadata": {
        "id": "HuR45aYF-_xx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ† Step 9: Check Fine-Tuning Results\n",
        "\n",
        "When fine-tuning completes, view all jobs and find the **ID** of the **Fine-Tuned Model**."
      ],
      "metadata": {
        "id": "_Lgkq1gI_DJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve a list of all fine-tuning jobs from the OpenAI API\n",
        "\n",
        "result = client.fine_tuning.jobs.list()\n",
        "result.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ft7lOnuPBax6",
        "outputId": "6a8eb14b-cf55-4638-cf3e-f330de4cf667"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[FineTuningJob(id='ftjob-XdWROGbBqI1j9vRFjDdX3jst', created_at=1731503980, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-3.5-turbo-0125:personal::AT7eFB0B', finished_at=1731504273, hyperparameters=Hyperparameters(n_epochs=15, batch_size=3, learning_rate_multiplier=0.3), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-mgfjI7HyLPZIak99Sy4VvIW5', result_files=['file-TCjIropahBX9R4BwZaqFjkwD'], seed=2061283848, status='succeeded', trained_tokens=25140, training_file='file-tkKgy3dx3YbXvm16SUEGRYgT', validation_file='file-g7CWUEgXa7PDgBvxZPUbZxc1', estimated_finish=None, integrations=[], user_provided_suffix=None),\n",
              " FineTuningJob(id='ftjob-HOzhngnH6aESL3yzmLq6vD98', created_at=1731497755, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-3.5-turbo-0125:personal::AT61Peal', finished_at=1731498021, hyperparameters=Hyperparameters(n_epochs=15, batch_size=3, learning_rate_multiplier=0.3), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-mgfjI7HyLPZIak99Sy4VvIW5', result_files=['file-MnvFqnZ2A7pMGPNrBMj35Vda'], seed=1745993970, status='succeeded', trained_tokens=25140, training_file='file-qfKCxKqvgTtJqEgPmYdqFolP', validation_file='file-xNsOQrMipvoUVT8BqytIT8IF', estimated_finish=None, integrations=[], user_provided_suffix=None),\n",
              " FineTuningJob(id='ftjob-EEzs9MqqAy53nnpQ5eJt6Hdi', created_at=1731497340, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-3.5-turbo-0125:personal::AT5ujVbJ', finished_at=1731497607, hyperparameters=Hyperparameters(n_epochs=15, batch_size=3, learning_rate_multiplier=0.3), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-mgfjI7HyLPZIak99Sy4VvIW5', result_files=['file-bw8xt7NiWBH0Krn4K6vnGWUi'], seed=630950835, status='succeeded', trained_tokens=25140, training_file='file-qfKCxKqvgTtJqEgPmYdqFolP', validation_file='file-xNsOQrMipvoUVT8BqytIT8IF', estimated_finish=None, integrations=[], user_provided_suffix=None),\n",
              " FineTuningJob(id='ftjob-tYQ8qGwhoAvSN4mfK8mCDehn', created_at=1731497324, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-3.5-turbo-0125:personal::AT5wF1M6', finished_at=1731497701, hyperparameters=Hyperparameters(n_epochs=15, batch_size=3, learning_rate_multiplier=0.3), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-mgfjI7HyLPZIak99Sy4VvIW5', result_files=['file-zpJmTLV0C4bUY0LULJksepft'], seed=818429110, status='succeeded', trained_tokens=25140, training_file='file-qfKCxKqvgTtJqEgPmYdqFolP', validation_file='file-xNsOQrMipvoUVT8BqytIT8IF', estimated_finish=None, integrations=[], user_provided_suffix=None),\n",
              " FineTuningJob(id='ftjob-FnSdcTx98w5ypCKS1EsiG1IT', created_at=1731497218, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-3.5-turbo-0125:personal::AT5uzL9i', finished_at=1731497622, hyperparameters=Hyperparameters(n_epochs=15, batch_size=3, learning_rate_multiplier=0.3), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-mgfjI7HyLPZIak99Sy4VvIW5', result_files=['file-9AW8y7fe0jwJ4c81bAcNb3dr'], seed=1104973238, status='succeeded', trained_tokens=25140, training_file='file-qfKCxKqvgTtJqEgPmYdqFolP', validation_file='file-xNsOQrMipvoUVT8BqytIT8IF', estimated_finish=None, integrations=[], user_provided_suffix=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **`client.fine_tuning.jobs.list()`**Â sends a request to the **OpenAI API** to fetch all the fine-tuning jobs that have been performed or are in progress. It returns a response object that contains various details about the fine-tuning jobs.\n",
        "- **`result`**Â : It is an object that contains aÂ **`.data`**Â attribute that holds the list of fine-tuning jobs.\n"
      ],
      "metadata": {
        "id": "DBGRXVMS_HpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the total number of fine-tuning jobs retrieved\n",
        "\n",
        "print(f\"Found {len(result.data)} Fine-Tuning Jobs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxtfjsfcDhBU",
        "outputId": "0b6e287b-d47a-48e3-9928-b3492abcedb6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5 Fine-Tuning Jobs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the fine-tuned model ID from the first fine-tuning job in the list\n",
        "fine_tuned_model_id = result.data[0].fine_tuned_model\n",
        "\n",
        "# Print the Fine-Tuned Model ID\n",
        "print(f\"Fine-Tuned Model ID: {fine_tuned_model_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUsBW2_SNaJ4",
        "outputId": "49e4541d-906a-4b03-b0ae-2573532a05f3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-Tuned Model ID: ft:gpt-3.5-turbo-0125:personal::AT7eFB0B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **`result.data[0]`**: This accesses the first fine-tuning job in the list of jobs (**`data`**Â is the list holding the jobs).\n",
        "- **`.fine_tuned_model`**: This accesses the specific attribute of that job which holds the information about the **fine-tuned model**.\n",
        "- `print(fine_tuned_model_id)`: prints the **ID** or details of the **fine-tuned model** that was extracted in the previous step.\n",
        "- **`ft:`**Â to indicate that itâ€™s a fine-tuned model"
      ],
      "metadata": {
        "id": "1cqIld6y_Ws3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ—£ Step 10: Test the Fine-Tuned Model\n",
        "\n",
        "Now, test your fine-tuned model by sending it a prompt. **Compare** its response to the base GPT-3.5 model."
      ],
      "metadata": {
        "id": "KfZol3TO_Y8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Base Model Response"
      ],
      "metadata": {
        "id": "QaR8APxD_gjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Send a prompt to the base (non-fine-tuned) model and get the response\n",
        "\n",
        "base_model_response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Ø¥Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø´Ø®ØµÙŠ Ø¨ØªØ¬Ø§ÙˆØ¨ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ø³ØªÙØ³Ø§Ø±Ø§ØªÙ‡Ù…\"},\n",
        "        {\"role\": \"user\", \"content\": \"Ø¥ÙŠÙ‡ Ø³ÙŠØ§Ø³Ø© Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„ÙƒØªØ¨ Ø¹Ù†Ø¯ÙƒÙ…ØŸ\"}\n",
        "        ]\n",
        ")\n",
        "\n",
        "print(\"Base Model Response:\")\n",
        "print(base_model_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1At7vT7NmR6",
        "outputId": "f08fe283-8568-44ef-b33f-aa89a926f3b8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base Model Response:\n",
            "Ø³ÙŠØ§Ø³Ø© Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„ÙƒØªØ¨ Ù„Ø¯Ù‰ Ù…Ø¹Ø¸Ù… Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª ØªØªØ¨Ø§ÙŠÙ† Ù‚Ù„ÙŠÙ„Ø§Ù‹ Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø¥Ù„Ù‰ Ø£Ø®Ø±Ù‰. ÙˆÙ„ÙƒÙ† Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù…ØŒ ÙŠÙ…ÙƒÙ† Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„ÙƒØªØ¨ ÙÙŠ Ù…Ø¹Ø¸Ù… Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø®Ù„Ø§Ù„ ÙØªØ±Ø© Ø²Ù…Ù†ÙŠØ© Ù…Ø¹ÙŠÙ†Ø© Ø¨Ø¹Ø¯ Ø´Ø±Ø§Ø¦Ù‡Ø§ Ù…Ù‚Ø§Ø¨Ù„ Ø§Ø³ØªØ±Ø¯Ø§Ø¯ Ù‚ÙŠÙ…ØªÙ‡Ø§ Ø£Ùˆ Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡Ø§ Ø¨ÙƒØªØ¨ Ø£Ø®Ø±Ù‰. ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙƒÙˆÙ† Ù‡Ù†Ø§Ùƒ Ø´Ø±ÙˆØ· Ù…Ø¹ÙŠÙ†Ø© Ù„Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø¥Ø±Ø¬Ø§Ø¹ Ù…Ø«Ù„ Ø­Ø§Ù„Ø© Ø§Ù„ÙƒØªØ§Ø¨ ÙˆØªÙˆØ§Ø¬Ø¯Ù‡ ÙÙŠ Ø­Ø§Ù„Ø© Ø¬ÙŠØ¯Ø©ØŒ ÙˆØ¹Ø§Ø¯Ø©Ù‹ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø¥ÙŠØµØ§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ Ù…ØªÙˆÙØ±Ù‹Ø§. Ù„Ø°Ø§ØŒ ÙŠÙØ¶Ù„ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø¥Ø±Ø¬Ø§Ø¹ Ù„Ø¯Ù‰ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ø§Ù„ØªÙŠ ØªØªØ³ÙˆÙ‚ Ù…Ù†Ù‡Ø§ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙØ§ØµÙŠÙ„ Ø¯Ù‚ÙŠÙ‚Ø© Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„ÙƒØªØ¨.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Fine-Tuned Model Response"
      ],
      "metadata": {
        "id": "xkQlN2pw_oK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Send the same prompt to the fine-tuned model and get the response\n",
        "\n",
        "fine_tuned_model_response = client.chat.completions.create(\n",
        "    model=fine_tuned_model_id,                # Fine-Tuned Model ID\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Ø¥Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø´Ø®ØµÙŠ Ø¨ØªØ¬Ø§ÙˆØ¨ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ø³ØªÙØ³Ø§Ø±Ø§ØªÙ‡Ù…\"},\n",
        "        {\"role\": \"user\", \"content\": \"Ø¥ÙŠÙ‡ Ø³ÙŠØ§Ø³Ø© Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„ÙƒØªØ¨ Ø¹Ù†Ø¯ÙƒÙ…ØŸ\"}\n",
        "        ]\n",
        ")\n",
        "\n",
        "print(\"Fine-Tuned Model Response:\")\n",
        "print(fine_tuned_model_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkUyjVnalMXy",
        "outputId": "e138f047-5e7b-448d-d8b6-132d6f717771"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-Tuned Model Response:\n",
            "Ù…Ø±Ø­Ø¨Ø§Ù‹! Ø³ÙŠØ§Ø³Ø© Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„ÙƒØªØ¨ Ø¹Ù†Ø¯Ù†Ø§ ØªØªØ¶Ù…Ù† Ù…Ø¯Ø© Ø¥Ø±Ø¬Ø§Ø¹ ØªØ¨Ù„Øº 14 ÙŠÙˆÙ…Ø§Ù‹ Ù…Ù† ØªØ§Ø±ÙŠØ® Ø§Ù„Ø´Ø±Ø§Ø¡ØŒ Ø¹Ù„Ù‰ Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„ÙƒØªØ¨ Ø¨Ø­Ø§Ù„ØªÙ‡Ø§ Ø§Ù„Ø£ØµÙ„ÙŠØ© ÙˆØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…Ø©. Ø¨Ù…Ø§ Ø£Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ Ø§ÙØªØ±Ø§Ø¶ÙŠØŒ Ù‚Ø¯ ØªØ­ØªØ§Ø¬ ØªØ£ÙƒÙŠØ¯ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ù…ÙƒØ§Ù† Ø§Ù„Ø´Ø±Ø§Ø¡ØŒ ÙØªØ£ÙƒØ¯ Ù…Ù† Ù…Ø±Ø§Ø¬Ø¹Ø© Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø¥Ø±Ø¬Ø§Ø¹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ø§Ù„Ø®Ø§Øµ Ø¨Ø§Ù„Ù…ØªØ¬Ø±. ØªØ­ØªØ§Ø¬ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø¨Ø´ÙŠØ¡ Ø¢Ø®Ø±ØŸ\n"
          ]
        }
      ]
    }
  ]
}